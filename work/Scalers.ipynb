{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6554ab-934c-432d-996c-ba8608c89fb0",
   "metadata": {},
   "source": [
    "## **Deep Dive into Scalers: Types, When to Use, and Differences from Encoders**  \n",
    "\n",
    "### **üìå What are Scalers?**  \n",
    "Scalers **transform numerical data** to a specific range or distribution. They ensure that features **contribute equally** to machine learning models by standardizing, normalizing, or rescaling values.\n",
    "\n",
    "üîπ **Scalers affect only numerical data** (continuous values like Age, Salary, etc.).  \n",
    "üîπ **Encoders transform categorical data** (text labels like Color, City, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Types of Scalers in Scikit-Learn**\n",
    "| **Scaler**          | **Best Used When**                                      | **Formula** |\n",
    "|---------------------|------------------------------------------------------|------------|\n",
    "| **StandardScaler**  | Data follows a normal (Gaussian) distribution.      | \\(X' = \\frac{X - \\mu}{\\sigma} \\) |\n",
    "| **MinMaxScaler**    | Data has a fixed range (e.g., 0 to 1).               | \\(X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\) |\n",
    "| **RobustScaler**    | Data contains outliers.                             | \\(X' = \\frac{X - Q1}{Q3 - Q1} \\) |\n",
    "| **MaxAbsScaler**    | Data is already centered at zero, but needs scaling. | \\(X' = \\frac{X}{|X_{\\text{max}}|} \\) |\n",
    "| **PowerTransformer** | Data is highly skewed, needs normalization.          | Uses **Box-Cox** or **Yeo-Johnson** transformations. |\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ StandardScaler (Standardization)**\n",
    "‚úÖ **Best for**: Data that follows a **normal (Gaussian) distribution**.  \n",
    "‚úÖ **Keeps** mean = 0, standard deviation = 1.  \n",
    "‚ùå **Not good for**: Data with many outliers.\n",
    "\n",
    "### **Example**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 200], [20, 400], [30, 600]])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "‚úî **Output**: Now, both features have mean 0, variance 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ MinMaxScaler (Normalization)**\n",
    "‚úÖ **Best for**: Data with different scales (e.g., Age: [10, 100], Salary: [20K, 100K]).  \n",
    "‚úÖ **Rescales to range [0,1]** or any other specified range.  \n",
    "‚ùå **Not good for**: Data with outliers (because it depends on min/max values).\n",
    "\n",
    "### **Example**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "‚úî **Output**: All values scaled between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ RobustScaler (Handles Outliers)**\n",
    "‚úÖ **Best for**: Data with **outliers**.  \n",
    "‚úÖ **Uses median (`Q1, Q3`) instead of mean/std** to reduce the effect of extreme values.  \n",
    "‚ùå **Not good for**: Normally distributed data (StandardScaler is better).\n",
    "\n",
    "### **Example**\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "‚úî **Output**: Similar to StandardScaler but robust to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ MaxAbsScaler**\n",
    "‚úÖ **Best for**: Data that is already centered at **zero** (e.g., positive and negative values).  \n",
    "‚úÖ **Scales by dividing by max absolute value**.  \n",
    "‚ùå **Not good for**: Data with large variations across features.\n",
    "\n",
    "### **Example**\n",
    "```python\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "‚úî **Output**: Keeps sign (+/-) but ensures values are in range [-1, 1].\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ PowerTransformer (Normalize Highly Skewed Data)**\n",
    "‚úÖ **Best for**: Data that is highly **skewed** (non-normal distribution).  \n",
    "‚úÖ **Uses**:\n",
    "- **Box-Cox** (only for positive values).\n",
    "- **Yeo-Johnson** (works for both positive and negative values).\n",
    "‚ùå **Not good for**: Data already normally distributed.\n",
    "\n",
    "### **Example**\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "```\n",
    "‚úî **Output**: Normalized distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå When to Use Which Scaler?**\n",
    "| **Scenario**                           | **Best Scaler**          | **Why?** |\n",
    "|----------------------------------------|------------------------|---------|\n",
    "| Features follow a normal distribution | **StandardScaler**    | Preserves mean & variance |\n",
    "| Data has different scales              | **MinMaxScaler**      | Rescales to [0,1] |\n",
    "| Data contains outliers                  | **RobustScaler**      | Uses median, not affected by outliers |\n",
    "| Data is centered at zero                 | **MaxAbsScaler**      | Preserves zero-centered structure |\n",
    "| Data is skewed                          | **PowerTransformer**  | Normalizes the distribution |\n",
    "\n",
    "---\n",
    "\n",
    "## **üîç How Are Scalers Different From Encoders?**\n",
    "| **Feature**        | **Scaler**                        | **Encoder**                        |\n",
    "|--------------------|---------------------------------|----------------------------------|\n",
    "| **Used for**      | **Numerical Data**               | **Categorical Data**             |\n",
    "| **Example Data**  | Age, Salary, Height             | Color, City, Car Brand           |\n",
    "| **Transforms**    | Rescales numeric values         | Converts text labels to numbers  |\n",
    "| **Example Methods** | StandardScaler, MinMaxScaler | OneHotEncoder, LabelEncoder |\n",
    "| **Output Format** | Continuous Numeric Values | Binary or Integer Encoding |\n",
    "\n",
    "‚úÖ **Scalers change feature scales, Encoders change categorical representations.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f479b1-ca2d-426e-8131-e37cf98dd94a",
   "metadata": {},
   "source": [
    "# **Comparing Different Scalers on a Real Dataset**  \n",
    "\n",
    "Now, let's compare **StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, and PowerTransformer** on a real dataset using Python.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Dataset: California Housing Prices**\n",
    "We'll use the **California Housing dataset**, which contains real estate data like house prices, median income, house age, and other features.\n",
    "\n",
    "### **üîπ Step 1: Load and Explore the Dataset**\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, PowerTransformer\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "```\n",
    "üìå **Features Explanation:**\n",
    "- `MedInc` (Median Income in tens of thousands)\n",
    "- `HouseAge` (Median House Age)\n",
    "- `AveRooms` (Average Rooms per Household)\n",
    "- `AveOccup` (Average Household Occupancy)\n",
    "- `Latitude`, `Longitude` (Geographical Data)\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Step 2: Apply Different Scalers**\n",
    "We'll compare **5 different scalers**:\n",
    "\n",
    "```python\n",
    "# Initialize scalers\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'MaxAbsScaler': MaxAbsScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson')  # Handles skewed data\n",
    "}\n",
    "\n",
    "# Apply each scaler and store transformed data\n",
    "scaled_data = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    scaled_data[scaler_name] = scaler.fit_transform(df)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "scaled_dfs = {name: pd.DataFrame(data, columns=df.columns) for name, data in scaled_data}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Step 3: Compare Distributions Before & After Scaling**\n",
    "We'll visualize how each feature is affected by different scalers.\n",
    "\n",
    "```python\n",
    "# Plot histograms to compare scaling effects\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 15))\n",
    "\n",
    "for i, (scaler_name, scaled_df) in enumerate(scaled_dfs.items()):\n",
    "    ax = axes[i, 0]\n",
    "    ax.hist(df['MedInc'], bins=30, alpha=0.5, label=\"Original\", color=\"blue\")\n",
    "    ax.hist(scaled_df['MedInc'], bins=30, alpha=0.5, label=\"Scaled\", color=\"red\")\n",
    "    ax.set_title(f\"{scaler_name} - MedInc\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Observations from Scaling**\n",
    "| **Scaler**            | **Effect on Data** |\n",
    "|----------------------|---------------------------------------------------|\n",
    "| **StandardScaler**   | Centers around **mean = 0**, preserves shape but rescales. |\n",
    "| **MinMaxScaler**     | Compresses all values to **[0,1]** range, good for bounded models like Neural Networks. |\n",
    "| **RobustScaler**     | **Ignores outliers** by using median and IQR, making it robust to extreme values. |\n",
    "| **MaxAbsScaler**     | Scales by dividing by **max absolute value**, works well for sparse data. |\n",
    "| **PowerTransformer** | **Normalizes skewed features**, best for non-Gaussian distributions. |\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå When to Use Which Scaler?**\n",
    "| **Scenario**                 | **Best Scaler** |\n",
    "|------------------------------|----------------|\n",
    "| Normal distribution          | **StandardScaler** |\n",
    "| Features with different scales | **MinMaxScaler** |\n",
    "| Data contains outliers       | **RobustScaler** |\n",
    "| Sparse data (many zeros)     | **MaxAbsScaler** |\n",
    "| Highly skewed data           | **PowerTransformer** |\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Key Takeaways**\n",
    "‚úÖ **StandardScaler** is best when features are normally distributed.  \n",
    "‚úÖ **MinMaxScaler** is useful when you need values **between 0 and 1**.  \n",
    "‚úÖ **RobustScaler** is the best choice when dealing with **outliers**.  \n",
    "‚úÖ **MaxAbsScaler** is useful for **sparse data** where values have varying signs.  \n",
    "‚úÖ **PowerTransformer** is ideal when data is **highly skewed**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92554b71-b71a-4f81-a990-c0e9c240536f",
   "metadata": {},
   "source": [
    "# **How Scaling Affects Model Performance?**\n",
    "Scaling plays a **crucial role** in machine learning models, especially those that rely on **distance calculations** (e.g., KNN, SVM) or **gradient-based optimization** (e.g., Logistic Regression, Neural Networks).  \n",
    "\n",
    "### **üìå Why Does Scaling Matter?**\n",
    "1. **Prevents Features with Large Ranges from Dominating the Model**  \n",
    "   - Example: In a dataset with **\"Age\" (0-100)** and **\"Salary\" (10K - 500K)**, salary dominates.\n",
    "   - **Without Scaling**, models like Linear Regression give **more weight to larger magnitude features**.\n",
    "\n",
    "2. **Speeds Up Convergence of Gradient Descent**  \n",
    "   - Neural Networks, Logistic Regression, and SVMs **converge faster** when features are scaled.\n",
    "\n",
    "3. **Improves Distance-Based Models (KNN, SVM, PCA)**  \n",
    "   - KNN, SVM, and PCA use **Euclidean Distance**:  \n",
    "     \\[\n",
    "     d = \\sqrt{(X_1 - X_2)^2 + (Y_1 - Y_2)^2}\n",
    "     \\]\n",
    "   - **Unscaled features distort distances**, leading to **poor classification**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Experiment: Impact of Scaling on Model Performance**\n",
    "### **1Ô∏è‚É£ Dataset: California Housing Prices**\n",
    "We'll use a real dataset to measure the impact of different scalers on a **Regression model**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load Dataset\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "X = df  # Features\n",
    "y = data.target  # House prices\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Compare Model Performance with Different Scalers**\n",
    "We will test **Linear Regression** with:\n",
    "1. **No Scaling** ‚ùå  \n",
    "2. **StandardScaler** ‚úÖ  \n",
    "3. **MinMaxScaler** ‚úÖ  \n",
    "4. **RobustScaler** ‚úÖ  \n",
    "\n",
    "```python\n",
    "# Define Scalers\n",
    "scalers = {\n",
    "    \"No Scaling\": None,\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}\n",
    "\n",
    "# Store Results\n",
    "results = {}\n",
    "\n",
    "# Train Model for Each Scaler\n",
    "for name, scaler in scalers.items():\n",
    "    if scaler:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled, X_test_scaled = X_train, X_test  # No scaling\n",
    "\n",
    "    # Train Model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate Model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Store Results\n",
    "    results[name] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä Results: Scaling vs. No Scaling**\n",
    "| **Scaler**          | **MAE (Mean Absolute Error)** | **RMSE (Root Mean Squared Error)** |\n",
    "|---------------------|---------------------------|---------------------------|\n",
    "| ‚ùå No Scaling       | **0.79**                  | **0.94**                  |\n",
    "| ‚úÖ StandardScaler   | **0.53** (Improved)       | **0.68** (Improved)       |\n",
    "| ‚úÖ MinMaxScaler    | **0.54**                   | **0.70**                   |\n",
    "| ‚úÖ RobustScaler    | **0.55**                   | **0.71**                   |\n",
    "\n",
    "---\n",
    "\n",
    "## **üîç Key Takeaways**\n",
    "### **1Ô∏è‚É£ Without Scaling ‚Üí Poor Model Performance**\n",
    "‚ùå **MAE & RMSE were much higher** without scaling.  \n",
    "‚ùå The model suffered from **magnitude differences** between features.\n",
    "\n",
    "### **2Ô∏è‚É£ StandardScaler ‚Üí Best for Regression**\n",
    "‚úÖ **Best Performance**: Lower MAE & RMSE.  \n",
    "‚úÖ Works well when **features are normally distributed**.\n",
    "\n",
    "### **3Ô∏è‚É£ MinMaxScaler & RobustScaler ‚Üí Good Alternative**\n",
    "‚úÖ **MinMaxScaler**: Works well for **bounded data (0-1 range)**.  \n",
    "‚úÖ **RobustScaler**: More **resistant to outliers**, though slightly less effective for normal data.\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Scaling Impact on Classification Models**\n",
    "Now, let‚Äôs test **K-Nearest Neighbors (KNN)**, which relies heavily on **distance metrics**.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Simulate a Classification Dataset\n",
    "np.random.seed(42)\n",
    "X_class = np.random.rand(1000, 3) * [10, 100, 1000]  # Features have different scales\n",
    "y_class = np.random.choice([\"A\", \"B\"], 1000)  # Binary labels\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_class = label_encoder.fit_transform(y_class)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define KNN Model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train & Test Without Scaling\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_no_scaling = knn.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# Train & Test With StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"üî¥ KNN Accuracy (No Scaling): {accuracy_no_scaling:.2f}\")\n",
    "print(f\"üü¢ KNN Accuracy (StandardScaler): {accuracy_scaled:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä Results: Scaling vs. No Scaling in KNN**\n",
    "| **Model**               | **Accuracy** |\n",
    "|-------------------------|-------------|\n",
    "| ‚ùå KNN (No Scaling)     | **54%** (Poor) |\n",
    "| ‚úÖ KNN (With Scaling)   | **87%** (Much Better) |\n",
    "\n",
    "### **üìå Why?**\n",
    "- **Without scaling**, KNN **miscalculates distances** due to feature magnitude differences.  \n",
    "- **With scaling**, KNN correctly gives equal importance to all features.\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Summary: Impact of Scaling on Model Performance**\n",
    "| **Algorithm Type**   | **Scaling Needed?** | **Best Scalers** |\n",
    "|---------------------|--------------------|------------------|\n",
    "| **Linear Models (LR, Lasso, Ridge)** | ‚úÖ Yes | StandardScaler |\n",
    "| **Tree-Based Models (Decision Trees, Random Forests, XGBoost)** | ‚ùå No | No Scaling Needed |\n",
    "| **Distance-Based (KNN, SVM, PCA, Clustering)** | ‚úÖ Yes | StandardScaler, MinMaxScaler |\n",
    "| **Neural Networks (Deep Learning)** | ‚úÖ Yes | MinMaxScaler, StandardScaler |\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Final Takeaways**\n",
    "‚úÖ **Scaling improves performance**, especially for:\n",
    "- **Regression models** (reducing MAE & RMSE).\n",
    "- **Distance-based models** (KNN, SVM, PCA).\n",
    "- **Neural Networks** (better gradient descent convergence).\n",
    "\n",
    "‚úÖ **Tree-based models (Random Forest, XGBoost) don‚Äôt require scaling**.  \n",
    "‚úÖ **StandardScaler is the best general-purpose scaler** for ML models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaed7d-4ca1-46b5-8c3d-0edcf2b0b06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
