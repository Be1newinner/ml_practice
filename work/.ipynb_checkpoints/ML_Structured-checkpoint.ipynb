{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e0c0d9-e3b5-463f-8d12-46aa009f73da",
   "metadata": {},
   "source": [
    "# Structured roadmap to AI ML:\n",
    "\n",
    "### **Phase 1: Foundations of AI/ML (2-3 Weeks)**\n",
    "- **Mathematics for ML:** Linear Algebra, Probability, Statistics, Calculus (Only required parts).\n",
    "- **Python for ML:** NumPy, Pandas, Matplotlib, Seaborn (for Data Handling & Visualization).\n",
    "- **Essential ML Concepts:** Supervised vs. Unsupervised Learning, Loss Functions, Optimization.\n",
    "\n",
    "### **Phase 2: Core Machine Learning (3-4 Weeks)**\n",
    "- **Scikit-Learn Basics:** Regression, Classification, Clustering, Dimensionality Reduction.\n",
    "- **Model Evaluation & Tuning:** Bias-Variance Tradeoff, Cross-validation, Hyperparameter Tuning.\n",
    "- **Feature Engineering & Selection.**\n",
    "\n",
    "### **Phase 3: Deep Learning (4-6 Weeks)**\n",
    "- **Neural Networks Basics:** Perceptron, Activation Functions, Backpropagation.\n",
    "- **Deep Learning Frameworks:** TensorFlow & PyTorch.\n",
    "- **CNNs for Image Processing, RNNs for Sequences, Transformers for NLP.**\n",
    "- **Optimization Techniques:** Adam, RMSProp, Learning Rate Scheduling.\n",
    "\n",
    "### **Phase 4: Advanced AI Topics (6-8 Weeks)**\n",
    "- **Unsu- **Python for ML:** NumPy, Pandas, Matplotlib, Seaborn (for Data Handling & Visualization).\n",
    "pervised Learning:** Autoencoders, GANs.\n",
    "- **Reinforcement Learning:** Q-Learning, Policy Gradients, Deep Q Networks.\n",
    "- **NLP & LLMs:** SpaCy, Hugging Face Transformers, Fine-tuning LLMs.\n",
    "- **MLOps & Deployment:** Model Serving with FastAPI, Docker, Kubernetes, CI/CD for ML.\n",
    "\n",
    "### **Phase 5: AI/ML Specialization & Projects (Ongoing)**\n",
    "- **AI in Backend Systems:** Recommendation Systems, Fraud Detection, Personalization.\n",
    "- **Real-World Projects:** Implement FAANG-style scalable AI services.\n",
    "- **AI Ethics & Responsible AI:** Bias, Fairness, Explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d61c9d-a441-491f-a23b-69e0df01f468",
   "metadata": {},
   "source": [
    "##  Phase 1: Foundations of AI/ML\n",
    "#### Mathematics for ML: Linear Algebra, Probability, Statistics, Calculus (Only required parts).\n",
    "#### Python for ML: NumPy, Pandas, Matplotlib, Seaborn (for Data Handling & Visualization).\n",
    "#### Essential ML Concepts: Supervised vs. Unsupervised Learning, Loss Functions, Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe2111-f9b0-44bf-ae74-0ac10efe0e0d",
   "metadata": {},
   "source": [
    "\n",
    "# **1. Supervised vs. Unsupervised Learning**  \n",
    "\n",
    "## **Supervised Learning**  \n",
    "Supervised learning is where we train a model using labeled data. This means we have both **input features (X)** and corresponding **output labels (Y)**. The model learns a mapping function from input to output.\n",
    "\n",
    "### **Key Concepts:**  \n",
    "- **Goal:** Learn a function that maps inputs to outputs: \\( f(X) \\rightarrow Y \\)\n",
    "- **Training Data:** Includes both inputs and correct outputs.\n",
    "- **Evaluation:** Model is tested on unseen data to check its generalization.\n",
    "\n",
    "### **Types of Supervised Learning:**  \n",
    "1. **Regression** (Continuous Output)  \n",
    "   - Predicts real-valued numbers.  \n",
    "   - Example: Predicting house prices, temperature forecasting.  \n",
    "   - Algorithms: Linear Regression, Ridge Regression, Decision Trees, Neural Networks.  \n",
    "\n",
    "2. **Classification** (Categorical Output)  \n",
    "   - Predicts class labels (e.g., spam or not spam, fraud detection).  \n",
    "   - Example: Image recognition, sentiment analysis.  \n",
    "   - Algorithms: Logistic Regression, SVM, Random Forest, Neural Networks.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Unsupervised Learning**  \n",
    "In unsupervised learning, we don’t have labeled output data. The model finds patterns or structures in the data.\n",
    "\n",
    "### **Key Concepts:**  \n",
    "- **Goal:** Discover hidden patterns or structures in data.\n",
    "- **Training Data:** Only contains input features (X), no labels (Y).\n",
    "- **Evaluation:** Harder to evaluate since we don’t have ground-truth labels.\n",
    "\n",
    "### **Types of Unsupervised Learning:**  \n",
    "1. **Clustering** (Grouping similar data points)  \n",
    "   - Example: Customer segmentation, topic modeling in NLP.  \n",
    "   - Algorithms: K-Means, DBSCAN, Hierarchical Clustering.  \n",
    "\n",
    "2. **Dimensionality Reduction** (Feature Extraction)  \n",
    "   - Example: Principal Component Analysis (PCA) for reducing image size.  \n",
    "   - Algorithms: PCA, t-SNE, Autoencoders.  \n",
    "\n",
    "**Supervised vs. Unsupervised Learning Comparison:**\n",
    "| Feature | Supervised Learning | Unsupervised Learning |\n",
    "|---------|---------------------|-----------------------|\n",
    "| Data Labels | Requires labeled data (X, Y) | Only input data (X) |\n",
    "| Goal | Learn to predict output | Discover hidden patterns |\n",
    "| Example | Spam detection, fraud prediction | Customer segmentation, Anomaly detection |\n",
    "| Common Algorithms | Linear Regression, Decision Trees, SVM, Neural Networks | K-Means, PCA, Autoencoders |\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Loss Functions**  \n",
    "Loss functions measure how well a model is performing by calculating the difference between **predicted output** and **actual output**. The goal of training is to minimize this loss.\n",
    "\n",
    "### **Loss Functions in Regression:**  \n",
    "- **Mean Squared Error (MSE):**  \n",
    "  \\[\n",
    "  MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
    "  \\]\n",
    "  - Penalizes larger errors more.\n",
    "  - Used in Linear Regression, Neural Networks.\n",
    "\n",
    "- **Mean Absolute Error (MAE):**  \n",
    "  \\[\n",
    "  MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|\n",
    "  \\]\n",
    "  - Less sensitive to outliers.\n",
    "\n",
    "- **Huber Loss (Combination of MSE & MAE):**  \n",
    "  - More robust to outliers.\n",
    "  - Used in advanced ML models.\n",
    "\n",
    "### **Loss Functions in Classification:**  \n",
    "- **Binary Cross-Entropy (Log Loss for 2 classes):**  \n",
    "  \\[\n",
    "  L = - \\frac{1}{N} \\sum \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "  \\]\n",
    "  - Used in Logistic Regression, Binary Classification.\n",
    "\n",
    "- **Categorical Cross-Entropy (for multi-class):**  \n",
    "  \\[\n",
    "  L = - \\sum y_i \\log(\\hat{y}_i)\n",
    "  \\]\n",
    "  - Used in Neural Networks.\n",
    "\n",
    "- **Hinge Loss (for SVMs):**  \n",
    "  \\[\n",
    "  L = \\sum \\max(0, 1 - y_i\\hat{y}_i)\n",
    "  \\]\n",
    "  - Used in Support Vector Machines (SVMs).\n",
    "\n",
    "### **Which Loss Function to Use?**  \n",
    "| Problem Type | Common Loss Function |\n",
    "|-------------|---------------------|\n",
    "| Regression | MSE, MAE, Huber Loss |\n",
    "| Binary Classification | Binary Cross-Entropy |\n",
    "| Multi-Class Classification | Categorical Cross-Entropy |\n",
    "| SVM Classification | Hinge Loss |\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Optimization (Training the Model)**\n",
    "Optimization is the process of minimizing the loss function by adjusting the model’s parameters.\n",
    "\n",
    "### **Optimization Algorithms:**\n",
    "1. **Gradient Descent (GD)**\n",
    "   - Iteratively updates weights to minimize loss.\n",
    "   - Formula:  \n",
    "     \\[\n",
    "     W = W - \\alpha \\frac{\\partial L}{\\partial W}\n",
    "     \\]\n",
    "   - **Challenges:** Can be slow for large datasets.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**\n",
    "   - Uses one sample at a time for faster updates.\n",
    "   - **Pros:** Faster than GD.\n",
    "   - **Cons:** Noisy convergence.\n",
    "\n",
    "3. **Mini-batch Gradient Descent**\n",
    "   - Uses a small batch of samples per iteration.\n",
    "   - **Balance between GD and SGD.**\n",
    "   - **Used in Deep Learning.**\n",
    "\n",
    "4. **Adam Optimizer (Adaptive Moment Estimation)**\n",
    "   - Combines momentum and adaptive learning rates.\n",
    "   - **Best for deep learning models.**\n",
    "   - Formula:  \n",
    "     \\[\n",
    "     m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "     \\]\n",
    "     \\[\n",
    "     v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "     \\]\n",
    "     \\[\n",
    "     \\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t\n",
    "     \\]\n",
    "   - **Used in:** Neural Networks, CNNs, Transformers.\n",
    "\n",
    "### **Comparison of Optimization Methods:**\n",
    "| Optimizer | Pros | Cons |\n",
    "|-----------|------|------|\n",
    "| Gradient Descent | Guarantees convergence | Slow for large datasets |\n",
    "| Stochastic Gradient Descent | Faster updates | Noisy updates |\n",
    "| Mini-batch GD | Balance of speed and accuracy | Needs tuning |\n",
    "| Adam | Best for deep learning | Uses more memory |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320b827-65ff-4068-9919-48ece289a480",
   "metadata": {},
   "source": [
    "# **Phase 2: Core Machine Learning (3-4 Weeks)**  \n",
    "\n",
    "## **Step 1: Scikit-Learn Basics**  \n",
    "\n",
    "Scikit-Learn is the most widely used Python library for traditional machine learning. We'll cover:  \n",
    "✅ **Regression** (Predicting continuous values)  \n",
    "✅ **Classification** (Predicting categories)  \n",
    "✅ **Clustering** (Grouping similar data points)  \n",
    "✅ **Dimensionality Reduction** (Feature compression)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Regression (Predicting Continuous Values)**  \n",
    "Regression is used when the target variable is continuous, like **house prices, sales forecasting, temperature prediction.**\n",
    "\n",
    "### **1.1. Types of Regression Models:**  \n",
    "| Model | Use Case | Pros | Cons |\n",
    "|--------|------------|------|------|\n",
    "| Linear Regression | Predict house prices | Simple, interpretable | Assumes linearity |\n",
    "| Ridge/Lasso Regression | Reduces overfitting | Handles multicollinearity | Needs tuning |\n",
    "| Decision Tree Regression | Non-linear trends | Captures complex patterns | Can overfit |\n",
    "| Random Forest Regression | More robust than decision trees | Handles large datasets | Slow for large data |\n",
    "| Gradient Boosting (XGBoost, LightGBM) | Best for Kaggle, FAANG ML models | High accuracy | Computationally expensive |\n",
    "\n",
    "### **1.2. Implementing Linear Regression in Scikit-Learn**  \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10  # Feature\n",
    "y = 2.5 * X + np.random.randn(100, 1) * 2  # Target with noise\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_test, y_test, color='blue', label=\"Actual\")\n",
    "plt.scatter(X_test, y_pred, color='red', label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "✅ **Key Takeaways:**  \n",
    "- `LinearRegression().fit(X_train, y_train)` trains the model.  \n",
    "- `mean_squared_error(y_test, y_pred)` evaluates the model.  \n",
    "- `plt.scatter()` visualizes actual vs. predicted values.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Classification (Predicting Categories)**  \n",
    "Classification is used when the target variable is categorical, like **spam detection, fraud detection, disease classification.**\n",
    "\n",
    "### **2.1. Types of Classification Models:**  \n",
    "| Model | Use Case | Pros | Cons |\n",
    "|--------|------------|------|------|\n",
    "| Logistic Regression | Email spam detection | Simple & interpretable | Assumes linearity |\n",
    "| Decision Trees | Fraud detection | Easy to understand | Can overfit |\n",
    "| Random Forest | Credit scoring | Reduces overfitting | Slow for large datasets |\n",
    "| Support Vector Machines (SVM) | Text classification | Effective in high dimensions | Expensive for big data |\n",
    "| Neural Networks | Image classification | Best accuracy | Requires large data |\n",
    "\n",
    "### **2.2. Implementing Logistic Regression in Scikit-Learn**  \n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "```\n",
    "✅ **Key Takeaways:**  \n",
    "- Logistic regression is used for multi-class problems (e.g., Iris dataset).  \n",
    "- Accuracy measures how well the model performs.  \n",
    "- `max_iter=200` ensures convergence in case of slow training.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Clustering (Unsupervised Learning - Finding Patterns)**  \n",
    "Clustering is used for **customer segmentation, anomaly detection, topic modeling.**\n",
    "\n",
    "### **3.1. Types of Clustering Algorithms:**  \n",
    "| Model | Use Case | Pros | Cons |\n",
    "|--------|------------|------|------|\n",
    "| K-Means | Customer segmentation | Fast, easy to implement | Needs k value |\n",
    "| DBSCAN | Anomaly detection | Handles noise, no need for k | Slow on large data |\n",
    "| Hierarchical | Market segmentation | Produces a dendrogram | Expensive for large data |\n",
    "\n",
    "### **3.2. Implementing K-Means Clustering**  \n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2) * 10\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot results\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=clusters, palette='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Dimensionality Reduction (Reducing Features for Efficiency)**  \n",
    "Dimensionality Reduction helps in **handling high-dimensional datasets** like images, text, genetics.\n",
    "\n",
    "### **4.1. Techniques for Dimensionality Reduction:**  \n",
    "| Model | Use Case | Pros | Cons |\n",
    "|--------|------------|------|------|\n",
    "| PCA | Reducing features | Fast, widely used | Loss of interpretability |\n",
    "| t-SNE | Visualizing high-dim data | Good for 2D plots | Computationally expensive |\n",
    "\n",
    "### **4.2. Implementing PCA**\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.7)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Transformation\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Model Evaluation & Tuning**  \n",
    "\n",
    "✅ **Bias-Variance Tradeoff:**  \n",
    "- **High Bias (Underfitting):** Model is too simple.  \n",
    "- **High Variance (Overfitting):** Model memorizes training data but fails on new data.  \n",
    "- **Solution:** Use **regularization, cross-validation, ensemble models.**  \n",
    "\n",
    "✅ **Cross-Validation:**  \n",
    "- Splits data into multiple training/testing sets.  \n",
    "- **K-Fold CV** is a common method.  \n",
    "\n",
    "✅ **Hyperparameter Tuning:**  \n",
    "- **Grid Search & Random Search:** Optimizes model parameters.  \n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model and hyperparameters\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
