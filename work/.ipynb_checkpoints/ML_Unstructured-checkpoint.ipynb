{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0aa3a9-6643-4d6f-8830-d76b8fcf432d",
   "metadata": {},
   "source": [
    "# Unstructured ML\n",
    "\n",
    "## **üîç What We Will Cover in Unstructured ML**\n",
    "Unstructured data requires specialized **preprocessing, feature extraction, and models**. We can divide this into:  \n",
    "\n",
    "### **1Ô∏è‚É£ Natural Language Processing (NLP) ‚Üí Text-Based ML**\n",
    "‚úÖ Tokenization, Stemming, Lemmatization  \n",
    "‚úÖ TF-IDF, Word Embeddings (Word2Vec, BERT)  \n",
    "‚úÖ Text Classification, Sentiment Analysis  \n",
    "‚úÖ Named Entity Recognition (NER), Topic Modeling  \n",
    "‚úÖ Chatbot Development (Rasa, OpenAI API, LLMs)  \n",
    "\n",
    "### **2Ô∏è‚É£ Computer Vision (CV) ‚Üí Image-Based ML**\n",
    "‚úÖ Image Preprocessing (OpenCV, PIL)  \n",
    "‚úÖ Feature Extraction (CNN, ResNet, Vision Transformers)  \n",
    "‚úÖ Image Classification, Object Detection  \n",
    "‚úÖ Face Recognition, OCR (Tesseract, EasyOCR)  \n",
    "\n",
    "### **3Ô∏è‚É£ Audio & Speech Processing**\n",
    "‚úÖ Feature Extraction (MFCC, Spectrograms)  \n",
    "‚úÖ Speech-to-Text (Whisper, DeepSpeech)  \n",
    "‚úÖ Voice Classification, Music Genre Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0040438-2476-41e9-94b7-889a6a308bb1",
   "metadata": {},
   "source": [
    "## **Unstructured ML** can indeed be divided into **Deep Learning** and **Shallow Learning** based on the **modeling techniques** used:\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ 1Ô∏è‚É£ Deep Learning in Unstructured ML**\n",
    "- **Deep Learning** refers to **neural networks with many layers**, capable of automatically learning complex patterns in unstructured data.\n",
    "  \n",
    "- **Common deep learning models for unstructured ML**:\n",
    "  - **CNNs (Convolutional Neural Networks)**: Used for **image data** (e.g., image classification, object detection).\n",
    "  - **RNNs (Recurrent Neural Networks)**, **LSTMs (Long Short-Term Memory)**, and **GRUs (Gated Recurrent Units)**: Used for **sequence data** (e.g., text, speech).\n",
    "  - **Transformers** (like **BERT, GPT**): Used for **NLP** tasks like sentiment analysis, text generation, translation, etc.\n",
    "  - **Autoencoders**: Used for **unsupervised tasks** like anomaly detection, image reconstruction, and denoising.\n",
    "\n",
    "#### **Deep Learning Examples in Unstructured ML:**\n",
    "- **Image Classification**: Using CNNs to classify images (e.g., cat vs dog).\n",
    "- **NLP**: Using RNNs, LSTMs, or transformers like BERT for text classification, translation, or summarization.\n",
    "- **Speech-to-Text**: Using deep neural networks to transcribe speech into text.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ 2Ô∏è‚É£ Shallow Learning in Unstructured ML**\n",
    "- **Shallow Learning** refers to **traditional machine learning algorithms** applied to unstructured data, often with manual feature extraction.\n",
    "\n",
    "- **Shallow models for unstructured ML** typically use **features engineered from the raw data**, such as using:\n",
    "  - **Handcrafted image features** (e.g., HOG, SIFT, or ORB) followed by traditional models like **SVM, Random Forest, or k-NN**.\n",
    "  - **Manual text features** (e.g., **TF-IDF** or **bag of words**) followed by traditional classifiers like **Naive Bayes**, **SVM**, or **Logistic Regression**.\n",
    "\n",
    "#### **Shallow Learning Examples in Unstructured ML:**\n",
    "- **Image Classification**: Using **handcrafted features** (e.g., HOG, SIFT) with **SVM** or **Random Forest**.\n",
    "- **Text Classification**: Using **TF-IDF features** with **Logistic Regression** or **Naive Bayes**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Summary:**\n",
    "- **Deep Learning**: Uses **neural networks with many layers** for **end-to-end learning** from raw unstructured data (e.g., raw images, raw text).\n",
    "- **Shallow Learning**: Requires **manual feature extraction** before applying traditional ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c567f-e026-4d44-a27e-415a3c132c04",
   "metadata": {},
   "source": [
    "## **üìå NLP Roadmap**\n",
    "We'll go step by step, from basic text processing to advanced **deep learning NLP models**.\n",
    "\n",
    "### **1Ô∏è‚É£ Text Preprocessing**\n",
    "‚úÖ Tokenization (**Splitting text into words/sentences**)  \n",
    "‚úÖ Stemming vs Lemmatization (**Reducing words to base form**)  \n",
    "‚úÖ Stopword Removal (**Filtering unnecessary words**)  \n",
    "‚úÖ Part-of-Speech (POS) Tagging (**Identifying word types**)  \n",
    "\n",
    "### **2Ô∏è‚É£ Feature Engineering for NLP**\n",
    "‚úÖ **TF-IDF (Term Frequency - Inverse Document Frequency)**  \n",
    "‚úÖ **Word Embeddings** (Word2Vec, GloVe, FastText)  \n",
    "‚úÖ **Transformers** (BERT, GPT)  \n",
    "\n",
    "### **3Ô∏è‚É£ NLP Applications**\n",
    "‚úÖ **Text Classification** (Spam Detection, Sentiment Analysis)  \n",
    "‚úÖ **Named Entity Recognition (NER)** (Extracting Names, Locations, Dates, etc.)  \n",
    "‚úÖ **Topic Modeling** (LDA, Latent Semantic Analysis)  \n",
    "\n",
    "### **4Ô∏è‚É£ Advanced NLP & Chatbot Development**\n",
    "‚úÖ **Intent Recognition** for chatbots  \n",
    "‚úÖ **Building AI Chatbots with Rasa & OpenAI API (GPT Models)**  \n",
    "‚úÖ **Deploying NLP Models (FastAPI, Flask, Streamlit)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274117f-010e-4872-90c0-ae510472a397",
   "metadata": {},
   "source": [
    "# **üöÄ Step 1: Basic NLP Preprocessing (Tokenization, Lemmatization, Stopwords Removal) using SpaCy**  \n",
    "\n",
    "## **üìå Why is Preprocessing Important?**\n",
    "Before applying machine learning models to text, we need to **clean and structure** the raw text to remove unnecessary noise and standardize words.  \n",
    "‚úÖ **Tokenization**: Breaking text into words/sentences.  \n",
    "‚úÖ **Lemmatization**: Converting words to their root form (better than stemming).  \n",
    "‚úÖ **Stopword Removal**: Removing common words that add little meaning (e.g., \"the\", \"is\").  \n",
    "‚úÖ **POS Tagging**: Identifying parts of speech (nouns, verbs, adjectives).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565e01be-2659-4ae6-951e-3f5afefc7114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6db93d7e-a362-4f11-948a-602dd889c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Tokens: ['SpaCy', 'is', 'an', 'amazing', 'NLP', 'library', '!', 'It', 'makes', 'text', 'processing', 'easy', '.']\n"
     ]
    }
   ],
   "source": [
    "## **1Ô∏è‚É£ Tokenization (Breaking Text into Words)**\n",
    "import spacy\n",
    "\n",
    "# Load the SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"SpaCy is an amazing NLP library! It makes text processing easy.\"\n",
    "\n",
    "# Process text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenization: Extract words\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"üîπ Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf43ca-6e6e-4a86-aa67-14158dca5d14",
   "metadata": {},
   "source": [
    "### **‚úÖ Output:**\n",
    "```\n",
    "üîπ Tokens: ['SpaCy', 'is', 'an', 'amazing', 'NLP', 'library', '!', 'It', 'makes', 'text', 'processing', 'easy', '.']\n",
    "```\n",
    "üí° **SpaCy automatically handles punctuation and special characters.**\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Lemmatization (Reducing Words to Root Form)**\n",
    "Lemmatization converts words to their **dictionary root** while keeping the meaning.  \n",
    "For example, `\"running\"` ‚Üí `\"run\"`, `\"better\"` ‚Üí `\"good\"`.\n",
    "\n",
    "```python\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"üîπ Lemmatized:\", lemmas)\n",
    "```\n",
    "### **‚úÖ Output:**\n",
    "```\n",
    "üîπ Lemmatized: ['SpaCy', 'be', 'an', 'amazing', 'NLP', 'library', '!', 'it', 'make', 'text', 'process', 'easy', '.']\n",
    "```\n",
    "üí° **\"is\" ‚Üí \"be\"** and **\"makes\" ‚Üí \"make\"** ‚Üí Shows how lemmatization standardizes words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ebe1b4-aae2-44da-8891-598bf8a5c664",
   "metadata": {},
   "source": [
    "## **3Ô∏è‚É£ Stopword Removal (Removing Unimportant Words)**\n",
    "Stopwords like `\"the\", \"is\", \"an\"` add little meaning to NLP models.  \n",
    "SpaCy has a **built-in stopword list**.\n",
    "\n",
    "```python\n",
    "# Removing stopwords\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "print(\"üîπ Tokens without Stopwords:\", filtered_tokens)\n",
    "```\n",
    "### **‚úÖ Output:**\n",
    "```\n",
    "üîπ Tokens without Stopwords: ['SpaCy', 'amazing', 'NLP', 'library', '!', 'makes', 'text', 'processing', 'easy', '.']\n",
    "```\n",
    "üí° **\"is\", \"an\", \"it\" were removed because they are stopwords.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d9e0c-ea2b-4fc4-ba4a-1647b1ee345b",
   "metadata": {},
   "source": [
    "## **4Ô∏è‚É£ POS Tagging (Identifying Parts of Speech)**\n",
    "Each word in a sentence has a role: **noun, verb, adjective, etc.**  \n",
    "This helps in tasks like **Named Entity Recognition (NER), sentiment analysis**.\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ‚Üí {token.pos_}\")\n",
    "```\n",
    "### **‚úÖ Output:**\n",
    "```\n",
    "SpaCy ‚Üí PROPN\n",
    "is ‚Üí AUX\n",
    "an ‚Üí DET\n",
    "amazing ‚Üí ADJ\n",
    "NLP ‚Üí PROPN\n",
    "library ‚Üí NOUN\n",
    "! ‚Üí PUNCT\n",
    "It ‚Üí PRON\n",
    "makes ‚Üí VERB\n",
    "text ‚Üí NOUN\n",
    "processing ‚Üí NOUN\n",
    "easy ‚Üí ADJ\n",
    ". ‚Üí PUNCT\n",
    "```\n",
    "üí° **\"SpaCy\" is a Proper Noun (`PROPN`), \"makes\" is a Verb (`VERB`)** ‚Üí Useful for text understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078135f1-7929-4199-a295-8611219e77b5",
   "metadata": {},
   "source": [
    "## **üìå Combining All Preprocessing Steps**\n",
    "Let's apply **tokenization, lemmatization, stopword removal, and POS tagging** together.\n",
    "\n",
    "```python\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"SpaCy is an amazing NLP library! It makes text processing easy.\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(\"‚úÖ Processed Text:\", processed_text)\n",
    "```\n",
    "### **‚úÖ Output:**\n",
    "```\n",
    "‚úÖ Processed Text: ['SpaCy', 'amazing', 'NLP', 'library', 'make', 'text', 'process', 'easy']\n",
    "```\n",
    "üí° **Now the text is cleaned and ready for ML models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f183460-30e2-45a4-912d-a0047ff10f08",
   "metadata": {},
   "source": [
    "## **üöÄ Step 2: Feature Engineering for NLP**  \n",
    "\n",
    "Now that we have **cleaned and preprocessed text**, we need to **convert it into numerical representations** that ML models can understand.  \n",
    "---\n",
    "\n",
    "### **üìå NLP Feature Engineering Overview**\n",
    "#### **1Ô∏è‚É£ TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- Traditional **bag-of-words** approach that weighs words based on their importance in a document.  \n",
    "\n",
    "#### **2Ô∏è‚É£ Word Embeddings (Word2Vec, GloVe, FastText)**\n",
    "- **Dense vector representations** that capture the **semantic meaning of words**.  \n",
    "\n",
    "#### **3Ô∏è‚É£ Transformers (BERT, GPT)**\n",
    "- **Context-aware embeddings** that understand word relationships **based on full sentence context**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "‚úÖ **What is TF-IDF?**  \n",
    "- **Term Frequency (TF):** Measures how frequently a word appears in a document.  \n",
    "- **Inverse Document Frequency (IDF):** Downweights words that appear frequently in many documents (e.g., \"the\", \"is\").  \n",
    "- **TF-IDF** = TF √ó IDF ‚Üí Higher values mean **important words**, lower values mean **common words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93455e63-8051-4211-80ba-860ed3b20bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ai      deep     great        is  language  learning   machine  \\\n",
      "0  0.000000  0.000000  0.631745  0.373119  0.000000  0.480458  0.480458   \n",
      "1  0.000000  0.414541  0.000000  0.244835  0.000000  0.630538  0.315269   \n",
      "2  0.410747  0.000000  0.000000  0.242594  0.410747  0.000000  0.000000   \n",
      "\n",
      "    natural        of      part  processing    subset  \n",
      "0  0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "1  0.000000  0.315269  0.000000    0.000000  0.414541  \n",
      "2  0.410747  0.312384  0.410747    0.410747  0.000000  \n"
     ]
    }
   ],
   "source": [
    "#### **üîπ Example: Compute TF-IDF in Python**\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"Machine learning is great\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Natural language processing is part of AI\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform text data\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "print(df_tfidf)\n",
    "\n",
    "# ‚úÖ **TF-IDF captures important words and removes unnecessary ones!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee1dbd-b5e4-40b5-a60e-81ba9090c36b",
   "metadata": {},
   "source": [
    "\n",
    "### **2Ô∏è‚É£ Word Embeddings (Word2Vec, GloVe, FastText)**\n",
    "‚úÖ **What are Word Embeddings?**  \n",
    "- Unlike TF-IDF, **word embeddings capture relationships between words**.  \n",
    "- **Words with similar meanings have similar vectors.**  \n",
    "- Used in **deep learning models (RNNs, Transformers, etc.)**.\n",
    "\n",
    "#### **üîπ Example: Word2Vec (Train Your Own Embeddings)**\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "sentences = [\n",
    "    \"Machine learning is great\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Natural language processing is part of AI\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get word vector for \"machine\"\n",
    "print(w2v_model.wv[\"machine\"])\n",
    "```\n",
    "‚úÖ **Now words are represented as dense numerical vectors instead of simple word counts!**  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Transformers (BERT, GPT)**\n",
    "‚úÖ **What Makes Transformers Special?**\n",
    "- Unlike **Word2Vec**, which gives a single vector for each word, **BERT & GPT create different word vectors based on context.**  \n",
    "- Example:  \n",
    "  - \"Apple is a fruit\" ‚Üí `Apple` means a **fruit**.  \n",
    "  - \"Apple makes iPhones\" ‚Üí `Apple` means a **company**.  \n",
    "  - **BERT understands the difference!**\n",
    "\n",
    "#### **üîπ Example: Extracting BERT Embeddings**\n",
    "```python\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "text = \"Machine learning is amazing!\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings.shape)  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "```\n",
    "‚úÖ **Now words are represented with deep contextual understanding!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d409d-3062-4ad1-b68c-e0cce291a47d",
   "metadata": {},
   "source": [
    "# **üöÄ Step 3: NLP Applications**\n",
    "Now that we have **numerical representations of text (TF-IDF, Word Embeddings, BERT, etc.)**, let‚Äôs apply them to **real-world NLP tasks**:\n",
    "\n",
    "### ‚úÖ **Text Classification** (Spam Detection, Sentiment Analysis)  \n",
    "### ‚úÖ **Named Entity Recognition (NER)** (Extracting Names, Locations, Dates, etc.)  \n",
    "### ‚úÖ **Topic Modeling** (LDA, Latent Semantic Analysis)  \n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Text Classification (Spam Detection, Sentiment Analysis)**  \n",
    "Text classification assigns a **category** to a given text (e.g., **spam vs. not spam, positive vs. negative sentiment**).  \n",
    "We‚Äôll cover:  \n",
    "- **TF-IDF + Logistic Regression** (Shallow Learning)  \n",
    "- **Word2Vec + Deep Learning** (Deep Learning)  \n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Example 1: Spam Detection using TF-IDF + Logistic Regression**\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample dataset (Spam vs Not Spam)\n",
    "data = {\n",
    "    \"text\": [\"Win a free iPhone now!\", \"Meeting at 10 AM\", \"Congratulations! You won a lottery\", \n",
    "             \"Call me when you are free\", \"Get rich quick with this scheme!\"],\n",
    "    \"label\": [1, 0, 1, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "‚úÖ **Simple but effective for spam detection!**  \n",
    "\n",
    "---\n",
    "\n",
    "### **üîπ Example 2: Sentiment Analysis using Word2Vec + Deep Learning**\n",
    "```python\n",
    "import gensim\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "sentences = [\"I love this movie!\", \"This was a terrible experience\", \"Absolutely fantastic!\", \"Worst product ever\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "X = pad_sequences(X, padding=\"post\")\n",
    "\n",
    "# Word2Vec Embeddings\n",
    "word2vec = gensim.models.Word2Vec(sentences=[s.split() for s in sentences], vector_size=50, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to vectors\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 50))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec.wv:\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "\n",
    "# Build LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, weights=[embedding_matrix], trainable=False),\n",
    "    LSTM(10),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X, np.array(labels), epochs=10, verbose=1)\n",
    "\n",
    "# Predict sentiment\n",
    "print(\"Sentiment Prediction:\", model.predict(pad_sequences(tokenizer.texts_to_sequences([\"Amazing experience!\"]), maxlen=X.shape[1])))\n",
    "```\n",
    "‚úÖ **Deep learning models like LSTM are more powerful for sentiment analysis!**  \n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Named Entity Recognition (NER)**\n",
    "NER identifies **important entities** (names, locations, organizations, dates, etc.) in a sentence.  \n",
    "We will use **SpaCy for efficient NER**.\n",
    "\n",
    "### **üîπ Example: Extracting Names, Locations, and Dates using SpaCy**\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Elon Musk founded SpaceX in 2002 and Tesla in 2004. He was born in South Africa.\"\n",
    "\n",
    "# Apply NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ‚Üí {ent.label_}\")\n",
    "\n",
    "# Display named entities visually\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "```\n",
    "‚úÖ **Automatically extracts Elon Musk (PERSON), SpaceX (ORG), and 2002 (DATE)!**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Topic Modeling (LDA, Latent Semantic Analysis)**\n",
    "Topic Modeling helps **discover hidden topics in text**.  \n",
    "We‚Äôll use **LDA (Latent Dirichlet Allocation)** to find topics in a document.\n",
    "\n",
    "### **üîπ Example: Topic Modeling with LDA**\n",
    "```python\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Sample dataset\n",
    "documents = [\"I love machine learning and AI\", \"Deep learning is the future of AI\", \n",
    "             \"Natural language processing enables AI to understand text\"]\n",
    "\n",
    "# Tokenize and create dictionary\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "# Create term-document matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_docs]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Display topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx+1}: {topic}\")\n",
    "```\n",
    "‚úÖ **Extracts AI-related topics from text!**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec32b97-cd93-4fa2-928f-6e6b9d4fec14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
